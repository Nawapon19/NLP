{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPMXZ/Q6GCDopYZy+p3+iYk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nawapon19/NLP-Practice/blob/main/Text_Preprocessing_Practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Text Preprocessing**"
      ],
      "metadata": {
        "id": "9KUqoOzsCrmp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yPrKwcEOCZb_",
        "outputId": "dd0e9f08-a57e-4216-99ad-4624d537cc7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]   Package tagsets is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: textblob in /usr/local/lib/python3.10/dist-packages (0.17.1)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.10/dist-packages (from textblob) (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (4.66.1)\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]   Package conll2000 is already up-to-date!\n",
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n",
            "Finished.\n"
          ]
        }
      ],
      "source": [
        "# import necessary libraries\n",
        "import nltk\n",
        "import string\n",
        "import re\n",
        "import inflect\n",
        "\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import pos_tag, ne_chunk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('tagsets')\n",
        "nltk.download('words')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "\n",
        "# install and import TextBlob\n",
        "!pip install -U textblob\n",
        "!python -m textblob.download_corpora\n",
        "from textblob import TextBlob"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Text Lowercase:**\n",
        "lowercase the text to reduce the size of the vocabulary of text data."
      ],
      "metadata": {
        "id": "qzeiBUCNIfZY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def text_lowercase(text):\n",
        "  return text.lower()\n",
        "\n",
        "input_str = \"Hey, did you know that the summer break is coming? Amazing right !! It's only 5 more days !!\"\n",
        "text_lowercase(input_str)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "hSpXMXAREnuy",
        "outputId": "51291cbf-8317-46fb-fb8a-236c76a0d208"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"hey, did you know that the summer break is coming? amazing right !! it's only 5 more days !!\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Remove numbers:**\n",
        "either to remove numbers or convert the numbers into their textual representations."
      ],
      "metadata": {
        "id": "YteoabO_J08L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# remove numbers using regular expressions\n",
        "def remove_numbers(text):\n",
        "  # re.sub(): replace digit character with ''\n",
        "  # \\d: matches any Unicode decimal digit, includes [0-9], and also many other digit characters\n",
        "  result = re.sub(r'\\d+', '', text)\n",
        "  return result\n",
        "\n",
        "input_str = \"There are 3 balls in this bag, and 12 in the other one.\"\n",
        "remove_numbers(input_str)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "Wv1_3-EWJN7i",
        "outputId": "8202c62e-52cc-4d36-fff8-189a98f1aab5"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'There are  balls in this bag, and  in the other one.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# convert the numbers into words using inflect library\n",
        "p = inflect.engine()\n",
        "\n",
        "# define a function to convert numbers into words\n",
        "def convert_number(text):\n",
        "  # split string into list of words\n",
        "  temp_str = text.split()\n",
        "  # create empty list for new string\n",
        "  new_string = []\n",
        "\n",
        "  for word in temp_str:\n",
        "    # if word is digit, convert the digit to word number and append to new string\n",
        "    if word.isdigit():\n",
        "      temp = p.number_to_words(word)\n",
        "      new_string.append(temp)\n",
        "\n",
        "    else:\n",
        "      new_string.append(word)\n",
        "\n",
        "  # join words in new string list to form a new string\n",
        "  temp_str = ' '.join(new_string)\n",
        "\n",
        "  return temp_str\n",
        "\n",
        "input_str = 'There are 3 balls in this bag, and 12 in the other one.'\n",
        "convert_number(input_str)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "e8orVHHiRbZ8",
        "outputId": "8311b79c-be9e-48b4-aacb-703a88ee910a"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'There are three balls in this bag, and twelve in the other one.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Remove punctuation:**\n",
        "to remove different forms of the same word. If not, then been. been, been! will be treated separately."
      ],
      "metadata": {
        "id": "i2tOx8PiTV9U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define a function for removing punctuations using str.translate and str.maketrans for mapping\n",
        "def remove_punctuation(text):\n",
        "  # create a mapping for translation replace punctuation with '' and remove ''\n",
        "  translator = str.maketrans('', '', string.punctuation)\n",
        "  return text.translate(translator)\n",
        "\n",
        "input_str = \"Hey, did you know that the summer break is coming? Amazing right !! It's only 5 more days !!\"\n",
        "remove_punctuation(input_str)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "tDWyAgu3TQj7",
        "outputId": "ab9be505-eeb1-4a81-a2cf-9757039574bf"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hey did you know that the summer break is coming Amazing right  Its only 5 more days '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Remove whitespaces:**\n",
        "use the join and split function to remove all the white spaces in a string."
      ],
      "metadata": {
        "id": "ok-UYOZ0bEY3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define a function for removing whitespace from text\n",
        "def remove_whitespace(text):\n",
        "  return ' '.join(text.split())\n",
        "\n",
        "input_str = \"   we don't need   the given questions\"\n",
        "remove_whitespace(input_str)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "dOf992gxbAkp",
        "outputId": "1a9c09ed-ecbc-4f74-dbf1-660b6e735fa2"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"we don't need the given questions\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenization:**\n",
        "Token – Each “entity” that is a part of whatever was split up based on rules. For examples, each word is a token when a sentence is “tokenized” into words. Each sentence can also be a token, if you tokenized the sentences out of a paragraph."
      ],
      "metadata": {
        "id": "2eKO8IPqCpDk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenization using nltk sentence and word tokenization\n",
        "text = \"Natural language processing (NLP) is a field \" + \\\n",
        "       \"of computer science, artificial intelligence \" + \\\n",
        "       \"and computational linguistics concerned with \" + \\\n",
        "       \"the interactions between computers and human \" + \\\n",
        "       \"(natural) languages, and, in particular, \" + \\\n",
        "       \"concerned with programming computers to \" + \\\n",
        "       \"fruitfully process large natural language \" + \\\n",
        "       \"corpora. Challenges in natural language \" + \\\n",
        "       \"processing frequently involve natural \" + \\\n",
        "       \"language understanding, natural language\" + \\\n",
        "       \"generation frequently from formal, machine\" + \\\n",
        "       \"-readable logical forms), connecting language \" + \\\n",
        "       \"and machine perception, managing human-\" + \\\n",
        "       \"computer dialog systems, or some combination \" + \\\n",
        "       \"thereof.\"\n",
        "\n",
        "print(sent_tokenize(text))\n",
        "print(word_tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bkrqLay4C01c",
        "outputId": "945e1860-1f73-4ac5-d8bc-4f58bec4301b"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Natural language processing (NLP) is a field of computer science, artificial intelligence and computational linguistics concerned with the interactions between computers and human (natural) languages, and, in particular, concerned with programming computers to fruitfully process large natural language corpora.', 'Challenges in natural language processing frequently involve natural language understanding, natural languagegeneration frequently from formal, machine-readable logical forms), connecting language and machine perception, managing human-computer dialog systems, or some combination thereof.']\n",
            "['Natural', 'language', 'processing', '(', 'NLP', ')', 'is', 'a', 'field', 'of', 'computer', 'science', ',', 'artificial', 'intelligence', 'and', 'computational', 'linguistics', 'concerned', 'with', 'the', 'interactions', 'between', 'computers', 'and', 'human', '(', 'natural', ')', 'languages', ',', 'and', ',', 'in', 'particular', ',', 'concerned', 'with', 'programming', 'computers', 'to', 'fruitfully', 'process', 'large', 'natural', 'language', 'corpora', '.', 'Challenges', 'in', 'natural', 'language', 'processing', 'frequently', 'involve', 'natural', 'language', 'understanding', ',', 'natural', 'languagegeneration', 'frequently', 'from', 'formal', ',', 'machine-readable', 'logical', 'forms', ')', ',', 'connecting', 'language', 'and', 'machine', 'perception', ',', 'managing', 'human-computer', 'dialog', 'systems', ',', 'or', 'some', 'combination', 'thereof', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenization using TextBlob\n",
        "text = (\"Natural language processing (NLP) is a field \" +\n",
        "       \"of computer science, artificial intelligence \" +\n",
        "       \"and computational linguistics concerned with \" +\n",
        "       \"the interactions between computers and human \" +\n",
        "       \"(natural) languages, and, in particular, \" +\n",
        "       \"concerned with programming computers to \" +\n",
        "       \"fruitfully process large natural language \" +\n",
        "       \"corpora. Challenges in natural language \" +\n",
        "       \"processing frequently involve natural \" +\n",
        "       \"language understanding, natural language\" +\n",
        "       \"generation frequently from formal, machine\" +\n",
        "       \"-readable logical forms), connecting language \" +\n",
        "       \"and machine perception, managing human-\" +\n",
        "       \"computer dialog systems, or some combination \" +\n",
        "       \"thereof.\")\n",
        "\n",
        "# create a TextBlob object\n",
        "blob_object = TextBlob(text)\n",
        "\n",
        "# tokenize paragraph into sentences\n",
        "print(\"Sentence Tokenize: \", blob_object.sentences)\n",
        "\n",
        "# tokenize paragraph into words\n",
        "print(\"\\nWord Tokenize: \", blob_object.words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "myh5GWfRDO2V",
        "outputId": "e673d9ab-0f0a-48ec-db71-a18cc57f5b2d"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence Tokenize:  [Sentence(\"Natural language processing (NLP) is a field of computer science, artificial intelligence and computational linguistics concerned with the interactions between computers and human (natural) languages, and, in particular, concerned with programming computers to fruitfully process large natural language corpora.\"), Sentence(\"Challenges in natural language processing frequently involve natural language understanding, natural languagegeneration frequently from formal, machine-readable logical forms), connecting language and machine perception, managing human-computer dialog systems, or some combination thereof.\")]\n",
            "\n",
            "Word Tokenize:  ['Natural', 'language', 'processing', 'NLP', 'is', 'a', 'field', 'of', 'computer', 'science', 'artificial', 'intelligence', 'and', 'computational', 'linguistics', 'concerned', 'with', 'the', 'interactions', 'between', 'computers', 'and', 'human', 'natural', 'languages', 'and', 'in', 'particular', 'concerned', 'with', 'programming', 'computers', 'to', 'fruitfully', 'process', 'large', 'natural', 'language', 'corpora', 'Challenges', 'in', 'natural', 'language', 'processing', 'frequently', 'involve', 'natural', 'language', 'understanding', 'natural', 'languagegeneration', 'frequently', 'from', 'formal', 'machine-readable', 'logical', 'forms', 'connecting', 'language', 'and', 'machine', 'perception', 'managing', 'human-computer', 'dialog', 'systems', 'or', 'some', 'combination', 'thereof']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Remove default stopwords:** Stopwords are words that do not contribute to the meaning of a sentence. Hence, they can safely be removed without causing any change in the meaning of the sentence."
      ],
      "metadata": {
        "id": "V-9KzXmHcCnq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(stopwords.words('english'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "huQyEmkueI0k",
        "outputId": "fabcd74d-2013-4132-d17f-8c5c622630f2"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define a function for removing stopwords\n",
        "def remove_stopwords(text):\n",
        "  # load English stopwords from stopwords module to a set\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  # tokenize text\n",
        "  word_tokens = word_tokenize(text)\n",
        "\n",
        "  filtered_text = [word for word in word_tokens if word not in stop_words]\n",
        "\n",
        "  return filtered_text\n",
        "\n",
        "example_text = \"This is a sample sentence and we are going to remove the stopwords from this.\"\n",
        "remove_stopwords(example_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5w4m3wQb6cw",
        "outputId": "d3ccfba8-a056-4d92-fd82-035545c6df95"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['This', 'sample', 'sentence', 'going', 'remove', 'stopwords', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stemming:**\n",
        "Stemming is the process of getting the root form of a word. Stem or root is the part to which inflectional affixes (-ed, -ize, -de, -s, etc.) are added. The stem of a word is created by removing the prefix or suffix of a word. So, stemming a word may not result in actual words.\n",
        "\n",
        "There are mainly three algorithms for stemming. These are the Porter Stemmer, the Snowball Stemmer and the Lancaster Stemmer. Porter Stemmer is the most common among them."
      ],
      "metadata": {
        "id": "ArsthRoP3wJa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create stemmer object using PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# define a function for stemming\n",
        "def stem_words(text):\n",
        "  word_tokens = word_tokenize(text)\n",
        "  stems = [stemmer.stem(word) for word in word_tokens]\n",
        "\n",
        "  return stems\n",
        "\n",
        "text = 'data science uses scientific methods algorithms and many types of processes'\n",
        "stem_words(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pUSiKvfrfE6Q",
        "outputId": "9280d520-8072-44fb-f46d-1a4cc5a41a16"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['data',\n",
              " 'scienc',\n",
              " 'use',\n",
              " 'scientif',\n",
              " 'method',\n",
              " 'algorithm',\n",
              " 'and',\n",
              " 'mani',\n",
              " 'type',\n",
              " 'of',\n",
              " 'process']"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lemmatization:**\n",
        "Like stemming, lemmatization also converts a word to its root form. The only difference is that lemmatization ensures that the root word belongs to the language.\n",
        "\n",
        "In NLTK, the WordNetLemmatizer is used to get the lemmas of words. A context for the lemmatization is needed. So, add the part-of-speech as a parameter."
      ],
      "metadata": {
        "id": "9z-wL3hX5O9o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create lemmatizer object using WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# define a function for lemmatization\n",
        "def lemmatize_word(text):\n",
        "  word_tokens = word_tokenize(text)\n",
        "  # provide a context: part of speech as verb\n",
        "  lemmas = [lemmatizer.lemmatize(word, pos = 'v') for word in word_tokens]\n",
        "\n",
        "  return lemmas\n",
        "\n",
        "text = 'data science uses scientific methods algorithms and many types of processes'\n",
        "lemmatize_word(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5rCXiuH5LzQ",
        "outputId": "f79b9bc7-fbe4-4b27-d278-af804eea9c31"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['data',\n",
              " 'science',\n",
              " 'use',\n",
              " 'scientific',\n",
              " 'methods',\n",
              " 'algorithms',\n",
              " 'and',\n",
              " 'many',\n",
              " 'type',\n",
              " 'of',\n",
              " 'process']"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part of Speech Tagging:**\n",
        "The part of speech explains how a word is used in a sentence. In a sentence, a word can have different contexts and semantic meanings. The basic natural language processing models like bag-of-words fail to identify these relations between words."
      ],
      "metadata": {
        "id": "w6Izdr1h6inR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define a function for pos tagging\n",
        "def pos_tagging(text):\n",
        "  word_tokens = word_tokenize(text)\n",
        "  return pos_tag(word_tokens)\n",
        "\n",
        "text = \"You just gave me a scare\"\n",
        "pos_tagging(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNDoaGN06V6_",
        "outputId": "824c0242-7694-4a66-a06f-887cea556b34"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('You', 'PRP'),\n",
              " ('just', 'RB'),\n",
              " ('gave', 'VBD'),\n",
              " ('me', 'PRP'),\n",
              " ('a', 'DT'),\n",
              " ('scare', 'NN')]"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get the details of all the part of speech tags using the Penn Treebank tagset\n",
        "nltk.help.upenn_tagset('NN')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tg2vkpGt7olL",
        "outputId": "a30bb43c-8d0b-44c0-8848-1e46018ff3ea"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NN: noun, common, singular or mass\n",
            "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
            "    investment slide humour falloff slick wind hyena override subhumanity\n",
            "    machinist ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Chunking:**\n",
        "Chunking is the process of extracting phrases from unstructured text and more structure to it. It is also known as shallow parsing. It is done on top of Part of Speech tagging. It groups word into “chunks”, mainly of noun phrases. Chunking is done using regular expressions."
      ],
      "metadata": {
        "id": "PHt3qVpy8PSP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define a chunking function with text and regular expression representing grammar as parameter\n",
        "def chunking(text, grammar):\n",
        "  word_tokens = word_tokenize(text)\n",
        "\n",
        "  # label words with pos\n",
        "  word_pos = pos_tag(word_tokens)\n",
        "\n",
        "  # create a chunk parser using grammar\n",
        "  chunkParser = nltk.RegexpParser(grammar)\n",
        "\n",
        "  # test it on the list of word tokens with tagged pos\n",
        "  tree = chunkParser.parse(word_pos)\n",
        "\n",
        "  for subtree in tree.subtrees():\n",
        "    print(subtree)\n",
        "\n",
        "  #tree.draw()\n",
        "\n",
        "sentence = 'the little yellow bird is flying in the sky'\n",
        "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
        "# NP (Noun Phrase) chunk should be formed whenever the chunker finds an optional determiner (DT)\n",
        "# followed by any number of adjectives (JJ) and then a noun (NN)\n",
        "\n",
        "chunking(sentence, grammar)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mIr0uzQ17857",
        "outputId": "f5ead92a-20a8-422c-d24b-42c87d0a5775"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  (NP the/DT little/JJ yellow/JJ bird/NN)\n",
            "  is/VBZ\n",
            "  flying/VBG\n",
            "  in/IN\n",
            "  (NP the/DT sky/NN))\n",
            "(NP the/DT little/JJ yellow/JJ bird/NN)\n",
            "(NP the/DT sky/NN)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Screenshot-1072.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "7ZyV8sy-AWcU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Named Entity Recognition:**\n",
        "Named Entity Recognition is used to extract information from unstructured text. It is used to classify entities present in a text into categories like a person, organization, event, places, etc. It gives us detailed knowledge about the text and the relationships between the different entities."
      ],
      "metadata": {
        "id": "oeyLUmysAoxJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define a function for named entity recognition\n",
        "def named_entity_recognition(text):\n",
        "  word_tokens = word_tokenize(text)\n",
        "\n",
        "  # pos tagging\n",
        "  word_pos = pos_tag(word_tokens)\n",
        "\n",
        "  # tree of word entities\n",
        "  print(ne_chunk(word_pos))\n",
        "\n",
        "text = \"Bill works for GeeksforGeeks so he went to Delhi for a meetup.\"\n",
        "named_entity_recognition(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-jH-Lbt90p9",
        "outputId": "86b9e433-2831-43f2-b0a9-9b3443397cc3"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  (PERSON Bill/NNP)\n",
            "  works/VBZ\n",
            "  for/IN\n",
            "  (ORGANIZATION GeeksforGeeks/NNP)\n",
            "  so/RB\n",
            "  he/PRP\n",
            "  went/VBD\n",
            "  to/TO\n",
            "  (GPE Delhi/NNP)\n",
            "  for/IN\n",
            "  a/DT\n",
            "  meetup/NN\n",
            "  ./.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IEqSpKLBBt2F"
      },
      "execution_count": 50,
      "outputs": []
    }
  ]
}